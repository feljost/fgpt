{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f745e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "\n",
    "def model_inference(model, enc = tiktoken.get_encoding(\"gpt2\"), prompt = \"Hello\"):\n",
    "    # Inference\n",
    "    max_length = 50\n",
    "    tokens = enc.encode(prompt)  # encode a prompt\n",
    "    # add batch dimension and move to GPU\n",
    "    tokens = torch.tensor(tokens, dtype=torch.long, device=\"cuda\").unsqueeze(0)\n",
    "    x = tokens.to(\"cuda\")  # move to GPU\n",
    "\n",
    "\n",
    "    # generate tokens\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        logits, loss = model(x)  # (B, T, vocab_size)\n",
    "        logits = logits[\n",
    "            :, -1, :\n",
    "        ]  # take the last token's logits (B, vocab_size) --> we only care about the next token\n",
    "        probs = F.softmax(logits, dim=-1)  # convert to probabilities\n",
    "        # skipped: temperature and top-k sampling\n",
    "        next_token = torch.multinomial(\n",
    "            probs, num_samples=1\n",
    "        )  # sample from the distribution\n",
    "        x = torch.cat((x, next_token), dim=1)  # append the new token to the sequence\n",
    "    \n",
    "    decoded_output = enc.decode([token for token in x[0].tolist() if token <= 50257])\n",
    "    \n",
    "    return x, \" \".join(decoded_output.split())  # decode the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5f53dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import tiktoken\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "from model import GPT\n",
    "from model import GPTConfig\n",
    "from model import B, T\n",
    "from data import DataLoader\n",
    "from inference import model_inference\n",
    "from hellaswag import iterate_examples\n",
    "from hellaswag import render_example\n",
    "from hellaswag import get_most_likely_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a0d4088",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(50304, 1024)\n",
       "    (wpe): Embedding(1024, 1024)\n",
       "    (h): ModuleList(\n",
       "      (0-15): 16 x Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (c_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=50304, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GPT(GPTConfig())\n",
    "model.to(\"cuda\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08b0c9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load(\"model_weights_20250904_0642.pth\", map_location=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d631326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strip unwanted prefixes\n",
    "new_state_dict = {}\n",
    "for k, v in state_dict.items():\n",
    "    if k.startswith(\"_orig_mod.\"):\n",
    "        new_state_dict[k.replace(\"_orig_mod.\", \"\")] = v\n",
    "    elif k.startswith(\"module.\"):\n",
    "        new_state_dict[k.replace(\"module.\", \"\")] = v\n",
    "    else:\n",
    "        new_state_dict[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f34e6c65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(new_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0ffe319a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 7454,  2402,   257,   640,    11,  2935, 26674,   274,   561,   423,\n",
       "          30671,   262,  8222,   329,  3598,   812,    11,   290,   772,   788,\n",
       "            561,  1249,   683,   284,  3100,   329,  2241,    13,   198,  3198,\n",
       "            286,  2935, 26674,   274,   338,   749,  5863, 10288,   284,   262,\n",
       "          20715, 15895,  5442,  3223,  2084,   373,   326,   981, 31521,  7244,\n",
       "           1276,   307, 32478, 48451]], device='cuda:0'),\n",
       " \"Once upon a time, Descartes would have inhabited the forest for seven years, and even then would allow him to dig for himself. One of Descartes's most famous references to the Nobel Prize winning dark ago was that while detecting Adam must beEitherSPEC\")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_inference(model=model, prompt=\"Once upon a time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4127bd89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
