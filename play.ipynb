{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f745e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "\n",
    "def model_inference(model, enc = tiktoken.get_encoding(\"gpt2\"), prompt = \"Hello\"):\n",
    "    # Inference\n",
    "    max_length = 50\n",
    "    tokens = enc.encode(prompt)  # encode a prompt\n",
    "    # add batch dimension and move to GPU\n",
    "    tokens = torch.tensor(tokens, dtype=torch.long, device=\"cuda\").unsqueeze(0)\n",
    "    x = tokens.to(\"cuda\")  # move to GPU\n",
    "\n",
    "\n",
    "    # generate tokens\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        logits, loss = model(x)  # (B, T, vocab_size)\n",
    "        logits = logits[\n",
    "            :, -1, :\n",
    "        ]  # take the last token's logits (B, vocab_size) --> we only care about the next token\n",
    "        probs = F.softmax(logits, dim=-1)  # convert to probabilities\n",
    "        # skipped: temperature and top-k sampling\n",
    "        next_token = torch.multinomial(\n",
    "            probs, num_samples=1\n",
    "        )  # sample from the distribution\n",
    "        x = torch.cat((x, next_token), dim=1)  # append the new token to the sequence\n",
    "    \n",
    "    decoded_output = enc.decode([token for token in x[0].tolist() if token <= 50257])\n",
    "    \n",
    "    return x, \" \".join(decoded_output.split())  # decode the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5f53dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import tiktoken\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "from model import GPT\n",
    "from model import GPTConfig\n",
    "from model import B, T\n",
    "from data import DataLoader\n",
    "from inference import model_inference\n",
    "from hellaswag import iterate_examples\n",
    "from hellaswag import render_example\n",
    "from hellaswag import get_most_likely_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a0d4088",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(50304, 1024)\n",
       "    (wpe): Embedding(1024, 1024)\n",
       "    (h): ModuleList(\n",
       "      (0-15): 16 x Block(\n",
       "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (c_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=50304, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GPT(GPTConfig())\n",
    "model.to(\"cuda\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08b0c9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load(\"model_weights_20250905_1150.pth\", map_location=\"cuda\")\n",
    "\n",
    "# Strip unwanted prefixes\n",
    "new_state_dict = {}\n",
    "for k, v in state_dict.items():\n",
    "    if k.startswith(\"_orig_mod.\"):\n",
    "        new_state_dict[k.replace(\"_orig_mod.\", \"\")] = v\n",
    "    elif k.startswith(\"module.\"):\n",
    "        new_state_dict[k.replace(\"module.\", \"\")] = v\n",
    "    else:\n",
    "        new_state_dict[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f34e6c65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(new_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ffe319a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 7454,  2402,   257,   640,   878,   465,  1204,    13,   198, 13488,\n",
       "            355,  6283,   355,  2910,   393,  1692,  9791,   287,   262, 39385,\n",
       "             13,  1649,   777, 14705,   547,  1234,   287,   262, 16161,   416,\n",
       "           1793,    11,   340,  3636,   447,   247,    83,  4691,   514,  7471,\n",
       "            780,    11,   996,    11,   484,   561,  1239, 18188,   262,  6881,\n",
       "             13,  1119,   550,   356]], device='cuda:0'),\n",
       " 'Once upon a time before his life. waited as strange as blood or human beings in the cosmos. When these planets were put in the galaxy by God, it wouldnâ€™t serve us anymore because, though, they would never attain the universe. They had we')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_inference(model=model, prompt=\"Once upon a time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4127bd89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[40313,   318,   262,  3139,   286,  7431, 46157,    11,   262,  3139,\n",
       "            286,  5267,   577, 14144,   373,  2067,   351,  1295, 31412,   860,\n",
       "            290,   416, 45086, 10956,  1618,   264, 13207,  1312,    13,    68,\n",
       "             13,   663,   826, 12979,   262,  6232,    11,   262,  1748,   351,\n",
       "            663,  1388,  1910,   290,  3599,   262,  1748,   351,   663,  2802,\n",
       "           1499,   357,   896,   264,   528]], device='cuda:0'),\n",
       " 'Paris is the capital of Ram Igor, the capital of Novose Eye was started with placefactor 9 and by hypersigious organ shole i.e. its right afterwards the neighborhood, the city with its main market and starting the city with its mother country (its siz')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_inference(model=model, prompt=\"Paris is the capital of\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2308789",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a094239",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
